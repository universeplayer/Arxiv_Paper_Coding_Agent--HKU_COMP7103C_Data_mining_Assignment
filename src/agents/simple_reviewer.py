"""Simple Reviewer Agent for code quality assessment."""

from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Dict, Any, List

from ..core.api_pool import ParallelLLMManager
from ..core.config import get_settings

logger = logging.getLogger(__name__)


class SimpleReviewerAgent:
    """Agent that reviews generated code for quality and correctness."""

    def __init__(self, api_manager: ParallelLLMManager):
        self.api_manager = api_manager
        self.settings = get_settings()

    async def review(
        self,
        objective: str,
        implementation: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Review generated code files.

        Args:
            objective: User's original request
            implementation: Result from SimpleCoderAgent

        Returns:
            Dictionary with review results and quality assessment
        """
        logger.info("Starting code review phase")

        files = implementation.get("files", [])
        if not files:
            return {
                "success": False,
                "error": "No files to review"
            }

        # Review each file
        file_reviews = []
        for file_info in files:
            logger.info(f"Reviewing {file_info['path']}")

            review = await self._review_file(
                objective=objective,
                file_info=file_info
            )

            file_reviews.append(review)
            logger.info(f"âœ“ Reviewed {file_info['path']} - Score: {review['score']:.2f}")

        # Calculate overall quality score
        avg_score = sum(r["score"] for r in file_reviews) / len(file_reviews)

        # Identify issues
        all_issues = []
        for review in file_reviews:
            all_issues.extend(review.get("issues", []))

        # Generate overall assessment
        overall_assessment = await self._generate_overall_assessment(
            objective=objective,
            file_reviews=file_reviews,
            avg_score=avg_score
        )

        # Update README with review findings
        output_dir = implementation.get("output_dir")
        if output_dir:
            await self._update_readme_with_findings(
                output_dir=output_dir,
                file_reviews=file_reviews,
                avg_score=avg_score,
                all_issues=all_issues,
                overall_assessment=overall_assessment
            )

        return {
            "success": True,
            "quality_score": avg_score,
            "file_reviews": file_reviews,
            "total_issues": len(all_issues),
            "issues": all_issues,
            "assessment": overall_assessment,
            "passed": avg_score >= 0.7  # 70% threshold
        }

    async def _review_file(
        self,
        objective: str,
        file_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Review a single file.

        Args:
            objective: User's original request
            file_info: File metadata including path and content

        Returns:
            Review results for this file
        """
        # Read file content
        file_path = Path(file_info["full_path"])
        if not file_path.exists():
            return {
                "file": file_info["path"],
                "score": 0.0,
                "issues": [{"severity": "critical", "message": "File not found"}],
                "suggestions": []
            }

        content = file_path.read_text(encoding="utf-8")

        # Determine file type
        file_ext = file_path.suffix.lower()
        
        # é™åˆ¶å†…å®¹é•¿åº¦ä»¥é¿å…è¶…è¿‡ API é™åˆ¶
        # Qwen æ¨¡å‹é™åˆ¶ï¼š30720 tokensï¼Œçº¦ç­‰äº 20000 å­—ç¬¦ï¼ˆè€ƒè™‘ä¸­æ–‡ï¼‰
        # ä¿å®ˆèµ·è§ï¼Œé™åˆ¶åœ¨ 25000 å­—ç¬¦
        max_content_length = 25000
        is_truncated = False
        if len(content) > max_content_length:
            content = content[:max_content_length]
            is_truncated = True
            logger.warning(f"File {file_info['path']} truncated from {len(content)} to {max_content_length} chars")

        # Build review prompt
        system_prompt = self._get_reviewer_system_prompt(file_ext)
        
        truncation_note = "\n\nâš ï¸ æ³¨æ„ï¼šæ–‡ä»¶å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰ 25000 å­—ç¬¦è¿›è¡Œå®¡æŸ¥ã€‚" if is_truncated else ""

        user_prompt = f"""è¯·è¯„å®¡ä»¥ä¸‹ä»£ç æ–‡ä»¶:

æ–‡ä»¶å: {file_info['path']}
ä»»åŠ¡ç›®æ ‡: {objective}
æ–‡ä»¶è¯´æ˜: {file_info.get('description', 'N/A')}{truncation_note}

ä»£ç å†…å®¹:
```
{content}
```

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„å®¡:
1. **ä»£ç è´¨é‡** (0-100åˆ†):
   - å¯è¯»æ€§å’Œä»£ç é£æ ¼
   - ç»“æ„å’Œç»„ç»‡
   - å‘½åè§„èŒƒ
   - æ³¨é‡Šè´¨é‡

2. **åŠŸèƒ½å®Œæ•´æ€§** (0-100åˆ†):
   - æ˜¯å¦æ»¡è¶³ä»»åŠ¡è¦æ±‚
   - åŠŸèƒ½å®ç°çš„å®Œæ•´æ€§
   - è¾¹ç•Œæƒ…å†µå¤„ç†

3. **å¥å£®æ€§** (0-100åˆ†):
   - é”™è¯¯å¤„ç†
   - è¾“å…¥éªŒè¯
   - å¼‚å¸¸å¤„ç†

4. **æ€§èƒ½å’Œæœ€ä½³å®è·µ** (0-100åˆ†):
   - æ€§èƒ½ä¼˜åŒ–
   - éµå¾ªæœ€ä½³å®è·µ
   - å®‰å…¨æ€§è€ƒè™‘

è¾“å‡ºè¦æ±‚ï¼ˆåŠ¡å¿…ä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. åªè¾“å‡ºä¸€ä¸ª ```json ... ``` ä»£ç å—ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€æ³¨é‡Šæˆ–è¯´æ˜ã€‚
2. JSON å†…å…¨éƒ¨ä½¿ç”¨è‹±æ–‡æ ‡ç‚¹ï¼ˆ","ã€":"ã€"[]" ç­‰ï¼‰ï¼Œä¸è¦ä½¿ç”¨ä¸­æ–‡å¼•å·æˆ–é¡¿å·ã€‚
3. å­—ç¬¦ä¸²å¿…é¡»ä½¿ç”¨åŒå¼•å·ï¼Œå¸ƒå°”/æ•°å­—ä½¿ç”¨åŸç”Ÿ JSON è¯­æ³•ã€‚

JSON æ¨¡æ¿å¦‚ä¸‹ï¼Œè¯·ç›´æ¥å¡«å…¥å®é™…æ•°å€¼ï¼š
```json
{{
    "scores": {{
        "quality": <0-100>,
        "completeness": <0-100>,
        "robustness": <0-100>,
        "performance": <0-100>
    }},
    "issues": [
        {{"severity": "critical|warning|info", "message": "é—®é¢˜æè¿°", "line": <è¡Œå·æˆ– null>}}
    ],
    "suggestions": [
        "æ”¹è¿›å»ºè®®1",
        "æ”¹è¿›å»ºè®®2"
    ],
    "summary": "ç®€è¦æ€»ç»“"
}}
```"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # Get review using parallel API calls
        # gpt-5 ç³»åˆ—éœ€è¦æ›´å¤š tokensï¼ˆå†…éƒ¨æ¨ç†ä¼šæ¶ˆè€—å¤§é‡ tokensï¼‰
        is_gpt5 = "gpt-5" in self.settings.reviewer_model.lower()
        max_tokens = 4000 if is_gpt5 else 2000

        results = await self.api_manager.call_parallel(
            messages=messages,
            model=self.settings.reviewer_model,
            n_parallel=2,
            provider="openai",
            reasoning_effort="medium",  # gpt-5 ç³»åˆ—å®¡æŸ¥ä½¿ç”¨ä¸­ç­‰æ¨ç†
            max_tokens=max_tokens
        )

        if not results:
            return {
                "file": file_info["path"],
                "score": 0.5,
                "issues": [{"severity": "warning", "message": "Review failed"}],
                "suggestions": []
            }

        # Parse review result
        try:
            review_data = self._parse_review_result(results[0]["content"])
        except Exception as e:
            logger.error(f"Failed to parse review result: {e}")
            return {
                "file": file_info["path"],
                "score": 0.5,
                "issues": [],
                "suggestions": [],
                "raw_review": results[0]["content"]
            }

        # Calculate overall score (0-1 scale)
        scores = review_data.get("scores", {})
        avg_score = (
            scores.get("quality", 50) +
            scores.get("completeness", 50) +
            scores.get("robustness", 50) +
            scores.get("performance", 50)
        ) / 400.0

        return {
            "file": file_info["path"],
            "score": avg_score,
            "scores": scores,
            "issues": review_data.get("issues", []),
            "suggestions": review_data.get("suggestions", []),
            "summary": review_data.get("summary", "")
        }

    def _get_reviewer_system_prompt(self, file_ext: str) -> str:
        """Get system prompt for file reviewer.

        Args:
            file_ext: File extension (e.g., '.py', '.html')

        Returns:
            System prompt string
        """
        base_prompt = """ä½ æ˜¯èµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„è½¯ä»¶å·¥ç¨‹ç»éªŒã€‚

ä½ çš„èŒè´£æ˜¯:
1. ä»”ç»†å®¡æŸ¥ä»£ç è´¨é‡
2. è¯†åˆ«æ½œåœ¨é—®é¢˜å’Œæ”¹è¿›ç©ºé—´
3. æä¾›å»ºè®¾æ€§çš„åé¦ˆ
4. ç¡®ä¿ä»£ç æ»¡è¶³æœ€ä½³å®è·µ

å®¡æŸ¥æ ‡å‡†:
- ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
- åŠŸèƒ½å®Œæ•´æ€§å’Œæ­£ç¡®æ€§
- é”™è¯¯å¤„ç†å’Œå¥å£®æ€§
- æ€§èƒ½å’Œå®‰å…¨æ€§
- éµå¾ªè¯­è¨€/æ¡†æ¶æœ€ä½³å®è·µ"""

        if file_ext == ".py":
            return base_prompt + """

Python ç‰¹å®šå…³æ³¨ç‚¹:
- PEP 8 ä»£ç è§„èŒƒ
- ç±»å‹æ³¨è§£ä½¿ç”¨
- å¼‚å¸¸å¤„ç†
- æ–‡æ¡£å­—ç¬¦ä¸²è´¨é‡
- æ€§èƒ½ä¼˜åŒ–ï¼ˆåˆ—è¡¨æ¨å¯¼ã€ç”Ÿæˆå™¨ç­‰ï¼‰"""

        elif file_ext in [".html", ".css", ".js"]:
            return base_prompt + """

å‰ç«¯ç‰¹å®šå…³æ³¨ç‚¹:
- HTMLè¯­ä¹‰åŒ–å’Œå¯è®¿é—®æ€§
- CSSæ€§èƒ½å’Œæµè§ˆå™¨å…¼å®¹æ€§
- JavaScriptç°ä»£æ€§å’Œæœ€ä½³å®è·µ
- å“åº”å¼è®¾è®¡
- ç”¨æˆ·ä½“éªŒ"""

        return base_prompt

    def _parse_review_result(self, content: str) -> Dict[str, Any]:
        """Parse LLM review output.

        Args:
            content: Raw review content from LLM

        Returns:
            Parsed review data
        """
        # Try to extract JSON from content
        content = content.strip()

        # Remove markdown code blocks if present
        if content.startswith("```json"):
            content = content[7:]
        elif content.startswith("```"):
            content = content[3:]

        if content.endswith("```"):
            content = content[:-3]

        content = content.strip()

        # Parse JSON
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            # Fallback: try to find JSON object in text
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())

            # If all else fails, return default structure
            return {
                "scores": {
                    "quality": 50,
                    "completeness": 50,
                    "robustness": 50,
                    "performance": 50
                },
                "issues": [],
                "suggestions": [],
                "summary": "Review parsing failed"
            }

    async def _generate_overall_assessment(
        self,
        objective: str,
        file_reviews: List[Dict[str, Any]],
        avg_score: float
    ) -> str:
        """Generate overall assessment of the project.

        Args:
            objective: User's original request
            file_reviews: Reviews of all files
            avg_score: Average quality score

        Returns:
            Overall assessment text
        """
        # Count issues by severity
        critical_count = 0
        warning_count = 0
        info_count = 0

        for review in file_reviews:
            for issue in review.get("issues", []):
                severity = issue.get("severity", "info")
                if severity == "critical":
                    critical_count += 1
                elif severity == "warning":
                    warning_count += 1
                else:
                    info_count += 1

        # Generate assessment
        if avg_score >= 0.9:
            quality_level = "ä¼˜ç§€"
        elif avg_score >= 0.7:
            quality_level = "è‰¯å¥½"
        elif avg_score >= 0.5:
            quality_level = "åŠæ ¼"
        else:
            quality_level = "éœ€è¦æ”¹è¿›"

        assessment = f"""
ä»£ç è´¨é‡è¯„ä¼°: {quality_level} ({avg_score * 100:.1f}/100)

æ–‡ä»¶å®¡æŸ¥æ•°é‡: {len(file_reviews)}
ä¸¥é‡é—®é¢˜: {critical_count}
è­¦å‘Š: {warning_count}
æç¤º: {info_count}

æ€»ä½“è¯„ä»·:
"""

        if avg_score >= 0.8:
            assessment += "ä»£ç è´¨é‡å¾ˆé«˜ï¼Œæ»¡è¶³ç”Ÿäº§ç¯å¢ƒè¦æ±‚ã€‚"
        elif avg_score >= 0.6:
            assessment += "ä»£ç è´¨é‡è‰¯å¥½ï¼Œå»ºè®®æ ¹æ®åé¦ˆè¿›è¡Œå°å¹…ä¼˜åŒ–ã€‚"
        else:
            assessment += "ä»£ç éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ï¼Œè¯·å…³æ³¨ä¸¥é‡é—®é¢˜å’Œè­¦å‘Šã€‚"

        if critical_count > 0:
            assessment += f"\n\nâš ï¸ å‘ç° {critical_count} ä¸ªä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤ã€‚"

        return assessment.strip()

    async def _update_readme_with_findings(
        self,
        output_dir: Path,
        file_reviews: List[Dict[str, Any]],
        avg_score: float,
        all_issues: List[Dict[str, Any]],
        overall_assessment: str
    ) -> None:
        """Update README with review findings.

        Args:
            output_dir: Output directory path
            file_reviews: List of file review results
            avg_score: Average quality score
            all_issues: All issues found
            overall_assessment: Overall assessment text
        """
        readme_path = output_dir / "README.md"
        
        if not readme_path.exists():
            logger.warning(f"README not found at {readme_path}, skipping update")
            return

        try:
            # Read existing README
            existing_content = readme_path.read_text(encoding="utf-8")
            
            # Remove old review section if exists
            if "\n## å®¡æŸ¥æŠ¥å‘Š" in existing_content:
                existing_content = existing_content.split("\n## å®¡æŸ¥æŠ¥å‘Š")[0]
            
            # Build review section
            review_section = "\n\n## å®¡æŸ¥æŠ¥å‘Š\n\n"
            review_section += f"**å®¡æŸ¥æ—¶é—´**: {self._get_current_time()}\n\n"
            review_section += f"**è´¨é‡è¯„åˆ†**: {avg_score * 100:.1f}/100\n\n"
            review_section += f"### æ€»ä½“è¯„ä»·\n\n{overall_assessment}\n\n"
            
            # Add file-specific findings
            if file_reviews:
                review_section += "### æ–‡ä»¶å®¡æŸ¥è¯¦æƒ…\n\n"
                for review in file_reviews:
                    file_name = review.get("file", "unknown")
                    score = review.get("score", 0) * 100
                    review_section += f"#### {file_name} (è¯„åˆ†: {score:.1f}/100)\n\n"
                    
                    # Issues
                    issues = review.get("issues", [])
                    if issues:
                        review_section += "**å‘ç°çš„é—®é¢˜**:\n\n"
                        for issue in issues:
                            severity = issue.get("severity", "info")
                            message = issue.get("message", "")
                            line = issue.get("line")
                            severity_emoji = {"critical": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(severity, "âšª")
                            line_str = f" (è¡Œ {line})" if line else ""
                            review_section += f"- {severity_emoji} [{severity.upper()}]{line_str}: {message}\n"
                        review_section += "\n"
                    
                    # Suggestions
                    suggestions = review.get("suggestions", [])
                    if suggestions:
                        review_section += "**æ”¹è¿›å»ºè®®**:\n\n"
                        for suggestion in suggestions:
                            review_section += f"- ğŸ’¡ {suggestion}\n"
                        review_section += "\n"
            
            # Add improvement plan section
            if avg_score < 0.9:  # Only add if not perfect
                review_section += "\n### æ”¹è¿›è®¡åˆ’å»ºè®®\n\n"
                review_section += "åŸºäºä»¥ä¸Šå®¡æŸ¥å‘ç°çš„é—®é¢˜ï¼Œå»ºè®®åˆ¶å®šä»¥ä¸‹æ”¹è¿›è®¡åˆ’ï¼š\n\n"
                
                # Group issues by severity
                critical_issues = [i for i in all_issues if i.get("severity") == "critical"]
                warning_issues = [i for i in all_issues if i.get("severity") == "warning"]
                
                if critical_issues:
                    review_section += "**ä¼˜å…ˆçº§ 1 - ä¸¥é‡é—®é¢˜** (å¿…é¡»ä¿®å¤):\n\n"
                    for idx, issue in enumerate(critical_issues[:5], 1):
                        review_section += f"{idx}. {issue.get('message', '')}\n"
                    review_section += "\n"
                
                if warning_issues:
                    review_section += "**ä¼˜å…ˆçº§ 2 - è­¦å‘Šé—®é¢˜** (å»ºè®®ä¿®å¤):\n\n"
                    for idx, issue in enumerate(warning_issues[:5], 1):
                        review_section += f"{idx}. {issue.get('message', '')}\n"
                    review_section += "\n"
                
                review_section += "**ä¼˜å…ˆçº§ 3 - ä¼˜åŒ–å»ºè®®** (å¯é€‰):\n\n"
                review_section += "- ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£å®Œå–„\n"
                review_section += "- æ€§èƒ½ä¼˜åŒ–å’Œä»£ç é‡æ„\n"
                review_section += "- æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹\n\n"
            
            # Write updated README
            updated_content = existing_content.rstrip() + review_section
            readme_path.write_text(updated_content, encoding="utf-8")
            
            logger.info(f"âœ“ Updated README with review findings at {readme_path}")
            
        except Exception as e:
            logger.error(f"Failed to update README: {e}")
    
    def _get_current_time(self) -> str:
        """Get current time as formatted string."""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
